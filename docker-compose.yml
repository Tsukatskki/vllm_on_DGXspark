services:
  vllm:
    build:
      context: .
      dockerfile: Dockerfile
    image: local/vllm-openai:nightly-aarch64
    container_name: vllm-openai
    command:
      - --model
      - ${VLLM_MODEL}
      - --dtype
      - ${VLLM_DTYPE}
      - --tensor-parallel-size
      - "${VLLM_TENSOR_PARALLEL_SIZE}"
      - --download-dir
      - ${VLLM_DOWNLOAD_DIR}
    ports:
      - "${VLLM_PORT}:8000"
    environment:
      - TRITON_PTXAS_PATH=/opt/ptxas
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - ./data:/data
      - /usr/local/cuda/bin/ptxas:/opt/ptxas:ro
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    shm_size: "8g"
    restart: unless-stopped
